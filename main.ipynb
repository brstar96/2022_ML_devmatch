{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmers Devmatch 2022\n",
    "\n",
    "## 1. Fetch Dataset\n",
    "필요한 라이브러리 설치 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (1.13.0.dev20220527)\n",
      "Requirement already satisfied: torchvision in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (0.14.0a0+f0f8a3c)\n",
      "Requirement already satisfied: torchaudio in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: numpy in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: requests in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from requests->torchvision) (1.26.9)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/brstar96/Library/Python/3.8/lib/python/site-packages (from scikit-learn) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 KB\u001b[0m \u001b[31m701.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/brstar96/Library/Python/3.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tqdm-4.64.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device_type = 'cuda'\n",
    "\n",
    "if device_type == 'cuda':\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "elif device_type == 'mps':\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainDatasetBuilder():\n",
    "    def __init__(self, dataset_root = './dataset', ):\n",
    "        super(trainDatasetBuilder, self).__init__()\n",
    "        # encode class\n",
    "        dataset0_classes = os.listdir(os.path.join(dataset_root, 'dataset0/train/'))\n",
    "        dataset0_label_encoder = LabelEncoder()\n",
    "        self.dataset0_classes = dataset0_label_encoder.fit(dataset0_classes)\n",
    "\n",
    "        dataset1_classes = os.listdir(os.path.join(dataset_root, 'dataset1/train/'))\n",
    "        dataset1_classes\n",
    "        dataset1_label_encoder = LabelEncoder()\n",
    "        self.dataset1_classes = dataset1_label_encoder.fit(dataset1_classes)\n",
    "\n",
    "        # set path and fetch sample path\n",
    "        dataset0_dir = os.path.join(dataset_root, '/dataset0/')\n",
    "        dataset1_dir = os.path.join(dataset_root, '/dataset1/')\n",
    "        \n",
    "        self.dataset0_train_dir = dataset0_dir + 'train/'\n",
    "        self.dataset0_test_dir = dataset0_dir + 'test/'\n",
    "\n",
    "        self.dataset1_train_dir = dataset1_dir + 'train/'\n",
    "        self.dataset1_test_dir = dataset1_dir + 'test/'\n",
    "\n",
    "        # dataset0's training/test datasets & dataloader\n",
    "        dataset0_X_train, dataset0_y_train = self.create_dataset(self.dataset0_train_dir)\n",
    "        dataset0_y_train = dataset0_label_encoder.transform(dataset0_y_train)\n",
    "\n",
    "        dataset0_X_test, dataset0_y_test = self.create_dataset(self.dataset0_test_dir)\n",
    "        dataset0_y_test = dataset0_label_encoder.transform(dataset0_y_test)\n",
    "\n",
    "        self.dataset0_train_dataset = TensorDataset(torch.tensor(dataset0_X_train).float(), torch.from_numpy(dataset0_y_train))\n",
    "        self.dataset0_test_dataset = TensorDataset(torch.tensor(dataset0_X_test).float(), torch.from_numpy(dataset0_y_test))\n",
    "\n",
    "        self.dataset0_train_dataloader = DataLoader(self.dataset0_train_dataset, batch_size=batch_size)\n",
    "        self.dataset0_test_dataloader= DataLoader(self.dataset0_test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # dataset1's training datasets & dataloader\n",
    "        dataset1_X_train, dataset1_y_train = self.create_dataset(self.dataset1_train_dir)\n",
    "        dataset1_y_train = dataset1_label_encoder.transform(dataset1_y_train)\n",
    "\n",
    "        self.dataset1_train_dataset = TensorDataset(torch.tensor(dataset1_X_train).float(), torch.from_numpy(dataset1_y_train))\n",
    "        self.dataset1_train_dataloader = DataLoader(self.dataset1_train_dataset, batch_size=batch_size)\n",
    "\n",
    "        # preparing testset\n",
    "        \"\"\"\n",
    "        해당 셀의 코드 중 dataloader 부분의 `shuffle=False` 는 수정하면 안됩니다.\n",
    "        \"\"\"\n",
    "\n",
    "        dataset1_test_X = []\n",
    "        test_list = os.listdir(self.dataset1_test_dir)\n",
    "\n",
    "        for f in test_list:\n",
    "            temp = pd.read_csv(self.dataset1_test_dir + f)\n",
    "            dataset1_test_X.append(torch.from_numpy(temp.values))\n",
    "\n",
    "        dataset1_test_X = pad_sequence(dataset1_test_X, batch_first=True)\n",
    "        self.dataset1_test_dataset = TensorDataset(torch.Tensor(dataset1_test_X))\n",
    "        self.dataset1_test_dataloader = DataLoader(self.dataset1_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    def create_dataset(self, dataset_dir):\n",
    "        X, y = [], []\n",
    "        labels = os.listdir(dataset_dir)\n",
    "        labels = [file for file in labels if not file.startswith ('.')] #.DS_Store 제외\n",
    "\n",
    "        for label in labels:\n",
    "            file_list = os.listdir(dataset_dir + label + '/')\n",
    "            file_list = [file for file in file_list if not file.startswith ('.')] #.DS_Store 제외\n",
    "\n",
    "            for f in file_list:\n",
    "                temp = pd.read_csv(dataset_dir + label + '/' + f)\n",
    "                X.append(torch.from_numpy(temp.values))\n",
    "                y.append(label)\n",
    "            X = pad_sequence(X, batch_first=True)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define models\n",
    "- model 1: transformer\n",
    "- model 2: resnet-18, 34\n",
    "\n",
    "\n",
    "### 3.1 define transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 작성\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling \n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "        \n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "        \n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        softmax = nn.functional.softmax\n",
    "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, d_model=256, in_channels=12, nhead=4, dim_feedforward=1024, nlayers=4, n_class=1, dropout=0.1, dropout_other=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "    self.transformer_encoder2 = TransformerEncoder(encoder_layers, nlayers)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "    self.pos_encoder2 = PositionalEncoding(d_model, dropout)\n",
    "    self.self_att_pool = SelfAttentionPooling(d_model)\n",
    "    self.self_att_pool2 = SelfAttentionPooling(d_model)\n",
    "\n",
    "    self.decoder = nn.Sequential(nn.Linear(d_model, d_model), \n",
    "                                       nn.Dropout(dropout_other),\n",
    "                                       nn.Linear(d_model, d_model), \n",
    "                                       nn.Linear(d_model, 64))\n",
    "    self.decoder2 = nn.Sequential(nn.Linear(d_model, d_model), \n",
    "                                       nn.Dropout(dropout_other),\n",
    "                                      #  nn.Linear(d_model, d_model), \n",
    "                                       nn.Linear(d_model, 64))\n",
    "\n",
    "    self.top_layer1 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(in_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(in_channels, 32, kernel_size=50, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer2 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(32),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(32, 64, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer3 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(64),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(64, 128, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer4 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(128),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(128, d_model, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.bottom_linear = nn.Sequential(\n",
    "                                 nn.Linear(64, 32),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(32, n_class)\n",
    "                             )\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model//2, num_layers=1,\n",
    "                                batch_first=False, bidirectional=True)\n",
    "\n",
    "  def _get_clones(self, module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "  def forward(self, src):      \n",
    "    src = src.squeeze(1) # [batch, 12, 4096]\n",
    "    src = self.top_layer1(src)\n",
    "    src = self.top_layer2(src)\n",
    "    src = self.top_layer3(src)\n",
    "    src = self.top_layer4(src) # [batch, 256, 241]\n",
    "    src = src.permute(2, 0, 1) # [241, batch, 256]\n",
    "    \n",
    "    # Positinal embedding for ecg signal \n",
    "    src = self.pos_encoder(src) # [sequence, batch, embedding]\n",
    "    \n",
    "    # Encoding & attention, pool \n",
    "    output = self.transformer_encoder(src)\n",
    "    output = output.permute(1,0,2)\n",
    "    output = self.self_att_pool(output)\n",
    "    logits = self.decoder(output) # [10, 64]\n",
    "    \n",
    "    xc = self.bottom_linear(logits)\n",
    "      \n",
    "    return xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_encoder2): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.25, inplace=False)\n",
      "        (dropout2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (pos_encoder2): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (self_att_pool): SelfAttentionPooling(\n",
      "    (W): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (self_att_pool2): SelfAttentionPooling(\n",
      "    (W): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder2): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (1): Dropout(p=0.25, inplace=False)\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (top_layer1): Sequential(\n",
      "    (0): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv1d(12, 32, kernel_size=(50,), stride=(2,))\n",
      "  )\n",
      "  (top_layer2): Sequential(\n",
      "    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv1d(32, 64, kernel_size=(15,), stride=(2,))\n",
      "  )\n",
      "  (top_layer3): Sequential(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv1d(64, 128, kernel_size=(15,), stride=(2,))\n",
      "  )\n",
      "  (top_layer4): Sequential(\n",
      "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv1d(128, 256, kernel_size=(15,), stride=(2,))\n",
      "  )\n",
      "  (bottom_linear): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(256, 128, bidirectional=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_ = Transformer(d_model=256, in_channels=12, nhead=4, dim_feedforward=1024, nlayers=4, n_class=1, dropout=0.25, dropout_other=0.25).to(device)\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 define resnet-18, 34 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock1d, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(self, block, layers, input_channels=12, inplanes=64, num_classes=2):\n",
    "        super(ResNet1d, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.conv1 = nn.Conv1d(input_channels, self.inplanes, kernel_size=15, stride=2, padding=7, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlock1d, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(BasicBlock1d, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock1d, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock1d, 512, layers[3], stride=2)\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x1 = self.adaptiveavgpool(x)\n",
    "        x2 = self.adaptivemaxpool(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    model = ResNet1d(BasicBlock1d, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    model = ResNet1d(BasicBlock1d, [3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1d(\n",
      "  (conv1): Conv1d(6, 64, kernel_size=(15,), stride=(2,), padding=(7,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (adaptiveavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (adaptivemaxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=1024, out_features=15, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "ResNet1d(\n",
      "  (conv1): Conv1d(6, 64, kernel_size=(15,), stride=(2,), padding=(7,), bias=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(64, 128, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(128, 256, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock1d(\n",
      "      (conv1): Conv1d(256, 512, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)\n",
      "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock1d(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock1d(\n",
      "      (conv1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (conv2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
      "      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (adaptiveavgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (adaptivemaxpool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (fc): Linear(in_features=1024, out_features=15, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "_resnet18 = resnet18(input_channels=6, num_classes=15).to(device)\n",
    "_resnet34 = resnet34(input_channels=6, num_classes=15).to(device)\n",
    "print(_resnet18)\n",
    "print(_resnet34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model\n",
    "### 4.1 define hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hparams, optimizer, lr with lr_scheduler\n",
    "args = {'dataset_root' : './dataset',\n",
    "        'vis_type' : 'none',\n",
    "        'batch_size' : 256, \n",
    "        'lr' : 0.0001, \n",
    "        'epochs' : 500, \n",
    "        'dataset0_n_class' : 15, \n",
    "        'dataset1_n_class' : 11, \n",
    "        'input_channels' : 6, \n",
    "        'which_loss' : 'ce',\n",
    "        'which_optimizer' : 'adamw',\n",
    "        'scheduler_type' : 'ReduceLROnPlateau',\n",
    "        'model_type' : 'transformer' # {'transformer', 'resnet18', 'resnet34'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 define trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, device):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.step = 0\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_auc = 0.0\n",
    "        self.best_metric_epoch = -1\n",
    "\n",
    "        # Build Dataloader\n",
    "        self.datasetbuilder = trainDatasetBuilder(dataset_root = self.args['dataset_root'])\n",
    "        print('Batch size: {}, lr: {}'.format(self.args['batch_size'], self.args['lr']))\n",
    "        print(\"Data Loaded\")\n",
    "\n",
    "        # Create Model Instance & Send model to device\n",
    "        if self.args['model_type'] == 'transformer':\n",
    "            self.model = Transformer(d_model=256, in_channels=self.args['input_channels'], nhead=4, dim_feedforward=1024, nlayers=4, n_class=1, dropout=0.25, dropout_other=0.25).to(device)\n",
    "        elif self.args['model_type'] == 'resnet18':\n",
    "            self.model = resnet18(input_channels=self.args['input_channels'], num_classes=self.args['dataset0_n_class']).to(device)\n",
    "        elif self.args['model_type'] == 'resnet34':\n",
    "            self.model = resnet34(input_channels=self.args['input_channels'], num_classes=self.args['dataset0_n_class']).to(device)\n",
    "\n",
    "\n",
    "        # Define loss function & send to device \n",
    "        self.loss_function = self.lossbuilder(self.args['which_loss']).to(self.device)\n",
    "        \n",
    "        # Define Optimizer & scheduler\n",
    "        self.optimizer = self.define_optimizer(self.model, which_optimizer=self.args['which_optimizer'])\n",
    "        self.lr_scheduler = self.define_scheduler(self.optimizer, scheduler_type='ReduceLROnPlateau')\n",
    "        \n",
    "        # Track Model Params via Wandb\n",
    "        if args['vis_type'] == 'wandb':\n",
    "            wandb.watch(self.model)\n",
    "        \n",
    "        # Train model\n",
    "        self.train_model()\n",
    "\n",
    "        \n",
    "    def define_optimizer(self, model, which_optimizer):\n",
    "        if which_optimizer == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=self.args['lr'], weight_decay=1e-3)\n",
    "        elif which_optimizer == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=self.args['lr'], betas=([0.9, 0.98]), weight_decay=1e-6)\n",
    "        else:\n",
    "            raise NotImplementedError('Invalid optimizer type input.')\n",
    "\n",
    "        print('Defined optimizer: {}'.format(type(optimizer)))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def define_scheduler(self, optimizer, scheduler_type):        \n",
    "        if scheduler_type == 'StepLR':\n",
    "            scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "        elif scheduler_type == 'MultiStepLR':\n",
    "            scheduler = MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5)\n",
    "        elif scheduler_type == 'ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, cooldown=10)\n",
    "        elif scheduler_type == 'CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)\n",
    "        elif scheduler_type == 'CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "        elif scheduler_type == 'None':\n",
    "            print('No Scheduler Initialized.')\n",
    "            return None\n",
    "        else:\n",
    "            print('Invalid scheduler type input.')\n",
    "            raise NotImplementedError('LR Scheduler Error')\n",
    "\n",
    "        print('{} scheduler initialized.'.format(type(scheduler)))\n",
    "\n",
    "        return scheduler\n",
    "    \n",
    "\n",
    "    def lossbuilder(which_loss):\n",
    "        # Define Loss Functions\n",
    "        if which_loss == 'ce': # multi-class classification\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "        elif which_loss == 'bce_logit': # single(one-class) classification \n",
    "            loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            print('Invalid loss input.')\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        print('Defined loss function: {}'.format(type(loss)))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        # Epoch Training\n",
    "        for epoch_idx in tqdm(iterable=range(0, self.opt['epochs']), desc='Epoch Progress', leave=False, file=sys.stdout):\n",
    "            epoch_idx += 1\n",
    "\n",
    "            if self.args['setproctitle'] == True:\n",
    "                import setproctitle\n",
    "                setproctitle.setproctitle('cID:{}_{}_{}_Training..._epoch:{}of{}'.format(os.uname()[1], self.args['modelname'], self.args['version'], epoch_idx, self.opt['epochs']))\n",
    "\n",
    "            # Set model to train mode (allow gradient flows)\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Initialize optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Batch step\n",
    "            for b_idx, batch in enumerate(tqdm(iterable=self.datasetbuilder.train_loader, desc='Batch Progress', leave=False, file=sys.stdout), start=1):\n",
    "                self.step = b_idx\n",
    "\n",
    "                if self.train_data_opt['use_rri'] == True:\n",
    "                    ecg_signal, rri, ecg_label = batch\n",
    "                    ecg_signal, rri, ecg_label = ecg_signal.to(self.device), rri.to(self.device), ecg_label.to(self.device)\n",
    "                else:\n",
    "                    ecg_signal, ecg_label = batch\n",
    "                    ecg_signal, ecg_label = ecg_signal.to(self.device), ecg_label.to(self.device)  \n",
    "\n",
    "                # Create Model Prediction & loss calculation\n",
    "                if self.train_data_opt['use_rri'] == True:\n",
    "                    output = self.model(src=ecg_signal, src2=rri)\n",
    "                else:\n",
    "                    output = self.model(src=ecg_signal)\n",
    "                \n",
    "                loss = self.loss_function(output, ecg_label)\n",
    "                loss.backward()\n",
    "                epoch_loss += loss.item()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Logging Metrics\n",
    "            epoch_loss /= self.step\n",
    "            print(f\"epoch {epoch_idx} average loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Validate Model \n",
    "            if self.is_validation == True:\n",
    "                val_loss, avg_f1, avg_auc = self.validation_model(epoch_idx=epoch_idx, epoch_loss=epoch_loss)\n",
    "\n",
    "            # Update loss scheduler\n",
    "            if self.lr_scheduler is not None:\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print('Current lr: {:.5f}'.format(current_lr))\n",
    "                self.lr_scheduler.step(val_loss)\n",
    "\n",
    "            if self.args['vis_type'] == 'wandb':\n",
    "                wandb.summary['current epoch'] = epoch_idx\n",
    "                wandb.log({'training loss': epoch_loss})\n",
    "                wandb.log({'validation loss': val_loss})\n",
    "                wandb.log({'validation avg_f1': avg_f1})\n",
    "                wandb.log({'validation avg_auc': avg_auc})\n",
    "                wandb.log({'current lr': current_lr})\n",
    "                \n",
    "\n",
    "        print(\"Train completed, Best_metric - F1: {:.3f}, AUC: {:.3f}, at epoch: {}\".format(self.best_f1, self.best_auc, self.best_metric_epoch))\n",
    "        # self.plot_data(self.args)\n",
    "\n",
    "\n",
    "    def validation_model(self, epoch_idx, epoch_loss):\n",
    "        val_loss = 0\n",
    "        output_list, labels_list = [], []\n",
    "        self.val_data_opt = self.data_opt['test']\n",
    "\n",
    "        if (epoch_idx) % self.val_interval == 0:\n",
    "            self.model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for v_b_idx, val_data in enumerate(self.datasetbuilder.val_loader):\n",
    "                    self.val_step = v_b_idx\n",
    "                    if self.train_data_opt['use_rri'] == True:\n",
    "                        ecg_signal, rri, ecg_label = val_data\n",
    "                        ecg_signal, rri, ecg_label = ecg_signal.to(self.device), rri.to(self.device), ecg_label.to(self.device)\n",
    "                    else:\n",
    "                        ecg_signal, ecg_label = val_data\n",
    "                        ecg_signal, ecg_label = ecg_signal.to(self.device), ecg_label.to(self.device)\n",
    "                    \n",
    "                    # whether use rri feature\n",
    "                    if self.train_data_opt['use_rri'] == True:\n",
    "                        val_logit = self.model(src=ecg_signal, src2=rri)\n",
    "                    else:\n",
    "                        val_logit = self.model(src=ecg_signal)\n",
    "                    \n",
    "                    val_output = torch.sigmoid(val_logit)\n",
    "\n",
    "                    # Compute metric\n",
    "                    ecg_label = ecg_label.expand_as(val_logit)\n",
    "                    loss = self.loss_function(val_logit, ecg_label)\n",
    "                    val_loss +=  loss.item()\n",
    "                    output_list.append(val_output.data.cpu().numpy())\n",
    "                    labels_list.append(ecg_label.data.cpu().numpy())\n",
    "                \n",
    "                val_loss /= self.val_step\n",
    "                avg_f1, avg_auc = self.calculate_metrics(np.vstack(labels_list), np.vstack(output_list)) # avg_precisions, avg_recalls, \n",
    "                print('Validation - Loss: {:.3f}, F1-Score: {:.3f}, AUC: {:.3f}'.format(val_loss, avg_f1, avg_auc))\n",
    "                    \n",
    "                # Save best model \n",
    "                best = avg_f1 > self.best_f1 or avg_auc > self.best_auc\n",
    "                if best:\n",
    "                    self.best_f1 = avg_f1\n",
    "                    self.best_auc = avg_auc\n",
    "                    self.best_metric_epoch = epoch_idx\n",
    "\n",
    "                    self.save_model(save_dir=self.args['paths']['model_save_dir'], \n",
    "                                    model=self.model, \n",
    "                                    optimizer=self.optimizer, \n",
    "                                    epoch=epoch_idx, \n",
    "                                    f1_score=avg_f1, \n",
    "                                    auc=avg_auc, \n",
    "                                    loss=epoch_loss, \n",
    "                                    best=best)\n",
    "                \n",
    "                # Logging Metrics\n",
    "                print(f\"current epoch: {epoch_idx} current mean F1-score: {avg_f1:.4f}\" \n",
    "                      f\"\\nbest mean F1-score: {self.best_f1:.4f} \"\n",
    "                      f\"at epoch: {self.best_metric_epoch}\")\n",
    "\n",
    "                return val_loss, avg_f1, avg_auc\n",
    "\n",
    "    def save_model(self, save_dir, model, optimizer, epoch, f1_score: float, auc: float, loss: float, best=False):\n",
    "    \n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "        epoch = '{0:04d}'.format(epoch)\n",
    "        loss = '{:.3f}'.format(loss)\n",
    "        f1_score = '{:.3f}'.format(f1_score)\n",
    "        auc = '{:.3f}'.format(auc)\n",
    "\n",
    "        if best:\n",
    "            save_path = os.path.join(save_dir, \"best_f1{}_auc{}_epoch{}.pth\".format(f1_score, auc, self.best_metric_epoch))\n",
    "            torch.save(save_dict, save_path)\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "        print(\"Save model to {}\".format(save_path))\n",
    "\n",
    "    def get_lr(self, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation model (make submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset1의 test 데이터를 사용한 일반화 성능 확인\n",
    "\n",
    "model.eval()\n",
    "predicted = []\n",
    "with torch.no_grad():\n",
    "  for idx, x in enumerate(dataset1_test_dataloader):\n",
    "    x = x.permute(0, 2, 1).contiguous().to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x.float())\n",
    "\n",
    "    _, preds = torch.max(output, 1)\n",
    "    predicted.extend(preds.cpu().numpy())\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_preds = pd.DataFrame(predicted, columns=['predicted value'])\n",
    "pd_preds.to_csv('submission.csv')\n",
    "pd_preds.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625ed975c5c227b3f2eae146e27cca2a4bb3771cd391bb2881da7d9db4168805"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
