{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmers Devmatch 2022\n",
    "\n",
    "## 1. Fetch Dataset\n",
    "필요한 라이브러리 설치 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ecg/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import datetime\n",
    "import shutil\n",
    "import os.path as osp\n",
    "sys.path.append('./')\n",
    "from pytz import timezone\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "device_type = 'cuda'\n",
    "\n",
    "if device_type == 'cuda':\n",
    "    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "elif device_type == 'mps':\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainDatasetBuilder():\n",
    "    def __init__(self, args, dataset_root = './dataset', dataset1_cv=True, val_ratio=0.1):\n",
    "        super(trainDatasetBuilder, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # encode class\n",
    "        dataset0_classes = os.listdir(os.path.join(dataset_root, 'dataset0/train/'))\n",
    "        dataset0_label_encoder = LabelEncoder()\n",
    "        self.dataset0_classes = dataset0_label_encoder.fit(dataset0_classes)\n",
    "\n",
    "        dataset1_classes = os.listdir(os.path.join(dataset_root, 'dataset1/train/'))\n",
    "        dataset1_classes\n",
    "        dataset1_label_encoder = LabelEncoder()\n",
    "        self.dataset1_classes = dataset1_label_encoder.fit(dataset1_classes)\n",
    "\n",
    "        # set path and fetch sample path\n",
    "        dataset0_dir = os.path.join(dataset_root, 'dataset0/')\n",
    "        dataset1_dir = os.path.join(dataset_root, 'dataset1/')\n",
    "        \n",
    "        self.dataset0_train_dir = dataset0_dir + 'train/'\n",
    "        self.dataset0_test_dir = dataset0_dir + 'test/'\n",
    "\n",
    "        self.dataset1_train_dir = dataset1_dir + 'train/'\n",
    "        self.dataset1_test_dir = dataset1_dir + 'test/'\n",
    "\n",
    "        # dataset0's training/test datasets & dataloader\n",
    "        dataset0_X_train, dataset0_y_train = self.create_dataset(mode='train', dataset_dir=self.dataset0_train_dir)\n",
    "        dataset0_y_train = dataset0_label_encoder.transform(dataset0_y_train)\n",
    "\n",
    "        dataset0_X_test, dataset0_y_test = self.create_dataset(mode='train', dataset_dir=self.dataset0_test_dir)\n",
    "        dataset0_y_test = dataset0_label_encoder.transform(dataset0_y_test)\n",
    "\n",
    "        self.dataset0_train_dataset = TensorDataset(torch.tensor(dataset0_X_train).float(), torch.tensor(torch.from_numpy(dataset0_y_train), dtype=torch.long))\n",
    "        self.dataset0_test_dataset = TensorDataset(torch.tensor(dataset0_X_test).float(), torch.tensor(torch.from_numpy(dataset0_y_test), dtype=torch.long))\n",
    "\n",
    "        self.dataset0_train_dataloader = DataLoader(self.dataset0_train_dataset, batch_size=self.args['batch_size'])\n",
    "        self.dataset0_test_dataloader= DataLoader(self.dataset0_test_dataset, batch_size=self.args['batch_size'])\n",
    "\n",
    "        # dataset1's training datasets & dataloader\n",
    "        dataset1_X_train, dataset1_y_train = self.create_dataset(mode='train', dataset_dir=self.dataset1_train_dir)\n",
    "        dataset1_y_train = dataset1_label_encoder.transform(dataset1_y_train)\n",
    "\n",
    "\n",
    "        if dataset1_cv == True:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(dataset1_X_train, dataset1_y_train, test_size=val_ratio, random_state=42)\n",
    "            self.dataset1_train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(torch.from_numpy(y_train), dtype=torch.long))\n",
    "            self.dataset1_test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(torch.from_numpy(y_test), dtype=torch.long))\n",
    "\n",
    "            self.dataset1_train_dataloader = DataLoader(self.dataset1_train_dataset, batch_size=self.args['batch_size'])\n",
    "            self.dataset1_test_dataloader= DataLoader(self.dataset1_test_dataset, batch_size=self.args['batch_size'])\n",
    "        else:\n",
    "            self.dataset1_train_dataset = TensorDataset(torch.tensor(dataset1_X_train).float(), torch.tensor(torch.from_numpy(dataset1_y_train), dtype=torch.long))\n",
    "            self.dataset1_train_dataloader = DataLoader(self.dataset1_train_dataset, batch_size=self.args['batch_size'])\n",
    "    \n",
    "    def create_dataset(self, mode='train', dataset_dir=None):\n",
    "        if mode == 'train':\n",
    "            X_lst, y_lst = [], []\n",
    "            labels = os.listdir(dataset_dir)\n",
    "            labels = [file for file in labels if not file.startswith ('.')] #.DS_Store 제외\n",
    "\n",
    "            for label in labels:\n",
    "                file_list = os.listdir(dataset_dir + label + '/')\n",
    "                file_list = [file for file in file_list if not file.startswith ('.')] #.DS_Store 제외\n",
    "\n",
    "                for f in file_list:\n",
    "                    temp = pd.read_csv(dataset_dir + label + '/' + f)\n",
    "                    X_lst.append(torch.from_numpy(temp.values))\n",
    "                    y_lst.append(label)\n",
    "                X_data = pad_sequence(X_lst, batch_first=True)\n",
    "\n",
    "            return X_data, y_lst\n",
    "        else:\n",
    "            raise NotImplementedError('Invalid mode input')\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define models\n",
    "- model 1: transformer\n",
    "- model 2: resnet-18, 34\n",
    "\n",
    "\n",
    "### 3.1 define transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 작성\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling \n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "        \n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "        \n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        softmax = nn.functional.softmax\n",
    "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, d_model=256, in_channels=12, nhead=4, dim_feedforward=1024, nlayers=4, n_class=1, dropout=0.1, dropout_other=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "    encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "    self.transformer_encoder2 = TransformerEncoder(encoder_layers, nlayers)\n",
    "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "    self.pos_encoder2 = PositionalEncoding(d_model, dropout)\n",
    "    self.self_att_pool = SelfAttentionPooling(d_model)\n",
    "    self.self_att_pool2 = SelfAttentionPooling(d_model)\n",
    "\n",
    "    self.decoder = nn.Sequential(nn.Linear(d_model, d_model), \n",
    "                                       nn.Dropout(dropout_other),\n",
    "                                       nn.Linear(d_model, d_model), \n",
    "                                       nn.Linear(d_model, 64))\n",
    "    self.decoder2 = nn.Sequential(nn.Linear(d_model, d_model), \n",
    "                                       nn.Dropout(dropout_other),\n",
    "                                      #  nn.Linear(d_model, d_model), \n",
    "                                       nn.Linear(d_model, 64))\n",
    "\n",
    "    self.top_layer1 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(in_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(in_channels, 32, kernel_size=50, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer2 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(32),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(32, 64, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer3 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(64),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(64, 128, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.top_layer4 = nn.Sequential(\n",
    "                          nn.BatchNorm1d(128),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv1d(128, d_model, kernel_size=15, stride=2),\n",
    "                      )\n",
    "\n",
    "    self.bottom_linear = nn.Sequential(\n",
    "                                 nn.Linear(64, 32),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(32, n_class), \n",
    "                             )\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=d_model, hidden_size=d_model//2, num_layers=1,\n",
    "                                batch_first=False, bidirectional=True)\n",
    "\n",
    "  def _get_clones(self, module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "  def forward(self, src):   \n",
    "    src = src.permute(0, 2, 1)\n",
    "    src = src.squeeze(1) # [batch, 12, 4096]\n",
    "    src = self.top_layer1(src)\n",
    "    src = self.top_layer2(src)\n",
    "    src = self.top_layer3(src)\n",
    "    src = self.top_layer4(src) # [batch, 256, 241]\n",
    "    src = src.permute(2, 0, 1) # [241, batch, 256]\n",
    "    \n",
    "    # Positinal embedding for ecg signal \n",
    "    src = self.pos_encoder(src) # [sequence, batch, embedding]\n",
    "    \n",
    "    # Encoding & attention, pool \n",
    "    output = self.transformer_encoder(src)\n",
    "    output = output.permute(1,0,2)\n",
    "    output = self.self_att_pool(output)\n",
    "    output = self.decoder(output) # [10, 64]\n",
    "    \n",
    "    logits = self.bottom_linear(output)\n",
    "      \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 define resnet-18, 34 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock1d(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock1d, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=7, stride=stride, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet1d(nn.Module):\n",
    "    def __init__(self, block, layers, input_channels=12, inplanes=64, num_classes=2):\n",
    "        super(ResNet1d, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.conv1 = nn.Conv1d(input_channels, self.inplanes, kernel_size=15, stride=2, padding=7, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(BasicBlock1d, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(BasicBlock1d, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock1d, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock1d, 512, layers[3], stride=2)\n",
    "        self.adaptiveavgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.adaptivemaxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x1 = self.adaptiveavgpool(x)\n",
    "        x2 = self.adaptivemaxpool(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    model = ResNet1d(BasicBlock1d, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    model = ResNet1d(BasicBlock1d, [3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model for transfer learning\n",
    "`dataset0`을 이용해 `dataset1`을 전이학습하기 위한 best 모델을 훈련합니다. \n",
    "### 4.1 define hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hparams, optimizer, lr with lr_scheduler\n",
    "args = {'modelname':'devmatch_transformer', \n",
    "        'dataset_root' : '/home/mgl/project_src/2022devmatch/dataset',\n",
    "        'experiment_root':'./experiment',\n",
    "        'stage':'base', # {base, transfer_learning, both}\n",
    "        'vis_type' : 'none',\n",
    "        'batch_size' : 64, \n",
    "        'lr' : 0.0001, \n",
    "        'epochs' : 500, \n",
    "        'dataset0_n_class' : 15, \n",
    "        'dataset1_n_class' : 11, \n",
    "        'input_channels' : 6, \n",
    "        'which_loss' : 'ce',\n",
    "        'which_optimizer' : 'adamw',\n",
    "        'scheduler_type' : 'ReduceLROnPlateau',\n",
    "        'val_interval':1,\n",
    "        'model_type' : 'transformer' # {'transformer', 'resnet18', 'resnet34'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 define trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, device):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.step = 0\n",
    "        self.best_f1 = 0.0\n",
    "        self.best_auc = 0.0\n",
    "        self.best_metric_epoch = -1\n",
    "        self.set_experiment(self.args)\n",
    "\n",
    "        # Build Dataloader\n",
    "        self.datasetbuilder = trainDatasetBuilder(args=self.args, dataset_root = self.args['dataset_root'])\n",
    "        print('Batch size: {}, lr: {}'.format(self.args['batch_size'], self.args['lr']))\n",
    "        print(\"Data Loaded\")\n",
    "\n",
    "        # Create Model Instance & Send model to device\n",
    "        if self.args['model_type'] == 'transformer':\n",
    "            self.model = Transformer(d_model=128, in_channels=self.args['input_channels'], nhead=4, dim_feedforward=1024, nlayers=4, n_class=self.args['dataset0_n_class'], dropout=0.25, dropout_other=0.25).to(device)\n",
    "        elif self.args['model_type'] == 'resnet18':\n",
    "            self.model = resnet18(input_channels=self.args['input_channels'], num_classes=self.args['dataset0_n_class']).to(device)\n",
    "        elif self.args['model_type'] == 'resnet34':\n",
    "            self.model = resnet34(input_channels=self.args['input_channels'], num_classes=self.args['dataset0_n_class']).to(device)\n",
    "\n",
    "\n",
    "        # Define loss function & send to device \n",
    "        self.loss_function = self.lossbuilder(which_loss=self.args['which_loss']).to(self.device)\n",
    "        \n",
    "        # Define Optimizer & scheduler\n",
    "        self.optimizer = self.define_optimizer(self.model, which_optimizer=self.args['which_optimizer'])\n",
    "        self.lr_scheduler = self.define_scheduler(self.optimizer, scheduler_type='ReduceLROnPlateau')\n",
    "        \n",
    "        # Track Model Params via Wandb\n",
    "        if args['vis_type'] == 'wandb':\n",
    "            wandb.watch(self.model)\n",
    "        \n",
    "        # Train model\n",
    "        if self.args['stage'] == 'base':\n",
    "            self.train_model(train_dataloader=self.datasetbuilder.dataset0_train_dataloader, val_dataloader=self.datasetbuilder.dataset0_test_dataloader)\n",
    "        elif self.args['stage'] == 'transfer_learning':\n",
    "            self.train_model(train_dataloader=self.datasetbuilder.dataset1_train_dataloader, val_dataloader=self.datasetbuilder.dataset1_test_dataloader)\n",
    "        elif self.args['stage'] == 'both':\n",
    "            self.train_model(train_dataloader=self.datasetbuilder.dataset0_train_dataloader, val_dataloader=self.datasetbuilder.dataset0_test_dataloader)\n",
    "            self.train_model(train_dataloader=self.datasetbuilder.dataset1_train_dataloader, val_dataloader=self.datasetbuilder.dataset1_test_dataloader)\n",
    "        else:\n",
    "            raise NotImplementedError('Invalid stage input')\n",
    "\n",
    "    def set_experiment(self, args):\n",
    "        self.args = args\n",
    "        name = self.args['modelname']\n",
    "\n",
    "        now = datetime.datetime.now(timezone('Asia/Seoul'))\n",
    "        nowDate = now.strftime('%Y-%m-%d')\n",
    "        nowTime = now.strftime('%H-%M-%S')\n",
    "        current_time = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "        # create experiment root\n",
    "\n",
    "        root = osp.join(args['experiment_root'], name+'/{}'.format(current_time))\n",
    "        if osp.exists(root):\n",
    "            shutil.rmtree(root, True)\n",
    "\n",
    "\n",
    "    def train_model(self, train_dataloader, val_dataloader):\n",
    "        # Epoch Training\n",
    "        for epoch_idx in tqdm(iterable=range(0, self.args['epochs']), desc='Epoch Progress', leave=False, file=sys.stdout):\n",
    "            epoch_idx += 1\n",
    "\n",
    "            # Set model to train mode (allow gradient flows)\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "\n",
    "            # Initialize optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Batch step\n",
    "            for b_idx, batch in enumerate(tqdm(iterable=train_dataloader, desc='Batch Progress', leave=False), start=1):\n",
    "                self.step = b_idx\n",
    "\n",
    "                ecg_signal, ecg_label = batch\n",
    "                ecg_signal, ecg_label = ecg_signal.to(self.device), ecg_label.to(self.device)  \n",
    "\n",
    "                # Create Model Prediction & loss calculation\n",
    "                output = self.model(src=ecg_signal) # [64, 15]\n",
    "\n",
    "                loss = self.loss_function(output, ecg_label)\n",
    "                loss.backward()\n",
    "                epoch_loss += loss.item()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Logging Metrics\n",
    "            epoch_loss /= self.step\n",
    "            print(f\"epoch {epoch_idx} average loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Validate Model \n",
    "            val_loss, avg_f1 = self.validation_model(epoch_idx=epoch_idx, epoch_loss=epoch_loss, dataloader=val_dataloader)\n",
    "\n",
    "            # Update loss scheduler\n",
    "            if self.lr_scheduler is not None:\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                print('Current lr: {:.5f}'.format(current_lr))\n",
    "                self.lr_scheduler.step(val_loss)\n",
    "\n",
    "            if self.args['vis_type'] == 'wandb':\n",
    "                wandb.summary['current epoch'] = epoch_idx\n",
    "                wandb.log({'training loss': epoch_loss})\n",
    "                wandb.log({'validation loss': val_loss})\n",
    "                wandb.log({'validation avg_f1': avg_f1})\n",
    "                wandb.log({'current lr': current_lr})\n",
    "                \n",
    "\n",
    "        print(\"Train completed, Best_metric - F1: {:.3f}, AUC: {:.3f}, at epoch: {}\".format(self.best_f1, self.best_auc, self.best_metric_epoch))\n",
    "        # self.plot_data(self.args)\n",
    "\n",
    "\n",
    "    def validation_model(self, epoch_idx, epoch_loss, dataloader):\n",
    "        val_loss = 0\n",
    "        val_f1_score = 0\n",
    "\n",
    "        if (epoch_idx) % self.args['val_interval'] == 0:\n",
    "            self.model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for v_b_idx, val_data in enumerate(dataloader):\n",
    "                    self.val_step = v_b_idx\n",
    "                    ecg_signal, ecg_label = val_data\n",
    "                    ecg_signal, ecg_label = ecg_signal.to(self.device), ecg_label.to(self.device)\n",
    "                    \n",
    "                    # whether use rri feature\n",
    "                    val_output = self.model(src=ecg_signal)  # data: [64, 15], label: [64]\n",
    "\n",
    "                    # Compute metric\n",
    "                    loss = self.loss_function(val_output, ecg_label)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(val_output, 1)\n",
    "                    print('sibal1', ecg_label.data.cpu().numpy().shape)\n",
    "                    print('sibal2', preds.data.cpu().numpy().shape)\n",
    "                    val_f1_score += f1_score(ecg_label.data.cpu().numpy(), preds.data.cpu().numpy())\n",
    "                    print('sibal score', val_f1_score)\n",
    "                \n",
    "                val_loss /= self.val_step\n",
    "                val_f1_score /= self.val_step\n",
    "                    \n",
    "                # Save best model \n",
    "                best = val_f1_score > self.best_f1 or val_f1_score > self.best_auc\n",
    "                if best:\n",
    "                    self.best_f1 = val_f1_score\n",
    "                    self.best_metric_epoch = epoch_idx\n",
    "\n",
    "                    self.save_model(save_dir=self.args['paths']['model_save_dir'], \n",
    "                                    model=self.model, \n",
    "                                    optimizer=self.optimizer, \n",
    "                                    epoch=epoch_idx, \n",
    "                                    f1_score=val_f1_score, \n",
    "                                    loss=epoch_loss, \n",
    "                                    best=best)\n",
    "                \n",
    "                # Logging Metrics\n",
    "                print(f\"current epoch: {epoch_idx} current mean F1-score: {val_f1_score:.4f}\" \n",
    "                      f\"\\nbest mean F1-score: {self.best_f1:.4f} \"\n",
    "                      f\"at epoch: {self.best_metric_epoch}\")\n",
    "\n",
    "                return val_loss, val_f1_score\n",
    "        \n",
    "\n",
    "    def define_optimizer(self, model, which_optimizer):\n",
    "        if which_optimizer == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=self.args['lr'], weight_decay=1e-3)\n",
    "        elif which_optimizer == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=self.args['lr'], betas=([0.9, 0.98]), weight_decay=1e-6)\n",
    "        else:\n",
    "            raise NotImplementedError('Invalid optimizer type input.')\n",
    "\n",
    "        print('Defined optimizer: {}'.format(type(optimizer)))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def define_scheduler(self, optimizer, scheduler_type):        \n",
    "        if scheduler_type == 'StepLR':\n",
    "            scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "        elif scheduler_type == 'MultiStepLR':\n",
    "            scheduler = MultiStepLR(optimizer, milestones=[200, 350], gamma=0.5)\n",
    "        elif scheduler_type == 'ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, cooldown=10)\n",
    "        elif scheduler_type == 'CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)\n",
    "        elif scheduler_type == 'CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "        elif scheduler_type == 'None':\n",
    "            print('No Scheduler Initialized.')\n",
    "            return None\n",
    "        else:\n",
    "            print('Invalid scheduler type input.')\n",
    "            raise NotImplementedError('LR Scheduler Error')\n",
    "\n",
    "        print('{} scheduler initialized.'.format(type(scheduler)))\n",
    "\n",
    "        return scheduler\n",
    "    \n",
    "\n",
    "    def lossbuilder(self, which_loss):\n",
    "        # Define Loss Functions\n",
    "        if which_loss == 'ce': # multi-class classification\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "        elif which_loss == 'bce_logit': # single(one-class) classification \n",
    "            loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            print('Invalid loss input.')\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        print('Defined loss function: {}'.format(type(loss)))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def save_model(self, save_dir, model, optimizer, epoch, f1_score: float, loss: float, best=False):\n",
    "    \n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "\n",
    "        epoch = '{0:04d}'.format(epoch)\n",
    "        loss = '{:.3f}'.format(loss)\n",
    "        f1_score = '{:.3f}'.format(f1_score)\n",
    "        auc = '{:.3f}'.format(auc)\n",
    "\n",
    "        if best:\n",
    "            save_path = os.path.join(save_dir, \"best_f1{}__epoch{}.pth\".format(f1_score, self.best_metric_epoch))\n",
    "            torch.save(save_dict, save_path)\n",
    "            print(\"Best model updated\")\n",
    "\n",
    "        print(\"Save model to {}\".format(save_path))\n",
    "\n",
    "    def get_lr(self, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758332/700563044.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset0_train_dataset = TensorDataset(torch.tensor(dataset0_X_train).float(), torch.tensor(torch.from_numpy(dataset0_y_train), dtype=torch.long))\n",
      "/tmp/ipykernel_1758332/700563044.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset0_test_dataset = TensorDataset(torch.tensor(dataset0_X_test).float(), torch.tensor(torch.from_numpy(dataset0_y_test), dtype=torch.long))\n",
      "/tmp/ipykernel_1758332/700563044.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset1_train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(torch.from_numpy(y_train), dtype=torch.long))\n",
      "/tmp/ipykernel_1758332/700563044.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.dataset1_test_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(torch.from_numpy(y_test), dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64, lr: 0.0001\n",
      "Data Loaded\n",
      "Defined loss function: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "Defined optimizer: <class 'torch.optim.adamw.AdamW'>\n",
      "<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'> scheduler initialized.\n",
      "Epoch Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1758332/2906386875.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 average loss: 2.5522\n",
      "                                                       \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label. It should be one of [0, 13]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mgl/project_src/2022devmatch/main.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000020vscode-remote?line=0'>1</a>\u001b[0m Trainer(args, device)\n",
      "\u001b[1;32m/home/mgl/project_src/2022devmatch/main.ipynb Cell 14'\u001b[0m in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=36'>37</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=38'>39</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_model(train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasetbuilder\u001b[39m.\u001b[39;49mdataset0_train_dataloader, val_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasetbuilder\u001b[39m.\u001b[39;49mdataset0_test_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=39'>40</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtransfer_learning\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=40'>41</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_model(train_dataloader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasetbuilder\u001b[39m.\u001b[39mdataset1_train_dataloader, val_dataloader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasetbuilder\u001b[39m.\u001b[39mdataset1_test_dataloader)\n",
      "\u001b[1;32m/home/mgl/project_src/2022devmatch/main.ipynb Cell 14'\u001b[0m in \u001b[0;36mTrainer.train_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_idx\u001b[39m}\u001b[39;00m\u001b[39m average loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=94'>95</a>\u001b[0m \u001b[39m# Validate Model \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=95'>96</a>\u001b[0m val_loss, avg_f1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidation_model(epoch_idx\u001b[39m=\u001b[39;49mepoch_idx, epoch_loss\u001b[39m=\u001b[39;49mepoch_loss, dataloader\u001b[39m=\u001b[39;49mval_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=97'>98</a>\u001b[0m \u001b[39m# Update loss scheduler\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/home/mgl/project_src/2022devmatch/main.ipynb Cell 14'\u001b[0m in \u001b[0;36mTrainer.validation_model\u001b[0;34m(self, epoch_idx, epoch_loss, dataloader)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=133'>134</a>\u001b[0m     val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=135'>136</a>\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(val_output, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=136'>137</a>\u001b[0m     val_f1_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m f1_score(ecg_label\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), preds\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=137'>138</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msibal score\u001b[39m\u001b[39m'\u001b[39m, val_f1_score)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22563130305f59485f64656275675f3232322e3132322e3230312e3734227d/home/mgl/project_src/2022devmatch/main.ipynb#ch0000015vscode-remote?line=139'>140</a>\u001b[0m val_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_step\n",
      "File \u001b[0;32m~/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1123\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=991'>992</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf1_score\u001b[39m(\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=992'>993</a>\u001b[0m     y_true,\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=993'>994</a>\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=999'>1000</a>\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1000'>1001</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1001'>1002</a>\u001b[0m     \u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1002'>1003</a>\u001b[0m \n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1003'>1004</a>\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1120'>1121</a>\u001b[0m \u001b[39m    modified with ``zero_division``.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1121'>1122</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1122'>1123</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1123'>1124</a>\u001b[0m         y_true,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1124'>1125</a>\u001b[0m         y_pred,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1125'>1126</a>\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1126'>1127</a>\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1127'>1128</a>\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1128'>1129</a>\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1129'>1130</a>\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1130'>1131</a>\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1131'>1132</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1261\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1134'>1135</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfbeta_score\u001b[39m(\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1135'>1136</a>\u001b[0m     y_true,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1136'>1137</a>\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1143'>1144</a>\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1144'>1145</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1145'>1146</a>\u001b[0m     \u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1146'>1147</a>\u001b[0m \n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1147'>1148</a>\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1257'>1258</a>\u001b[0m \u001b[39m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1258'>1259</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1260'>1261</a>\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1261'>1262</a>\u001b[0m         y_true,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1262'>1263</a>\u001b[0m         y_pred,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1263'>1264</a>\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1264'>1265</a>\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1265'>1266</a>\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1266'>1267</a>\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1267'>1268</a>\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1268'>1269</a>\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1269'>1270</a>\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1270'>1271</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1271'>1272</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1544\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1541'>1542</a>\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1542'>1543</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1543'>1544</a>\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1545'>1546</a>\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1546'>1547</a>\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1356\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1353'>1354</a>\u001b[0m     \u001b[39mif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m present_labels:\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1354'>1355</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(present_labels) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1355'>1356</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1356'>1357</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpos_label=\u001b[39m\u001b[39m{\u001b[39;00mpos_label\u001b[39m}\u001b[39;00m\u001b[39m is not a valid label. It \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1357'>1358</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshould be one of \u001b[39m\u001b[39m{\u001b[39;00mpresent_labels\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1358'>1359</a>\u001b[0m             )\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1359'>1360</a>\u001b[0m     labels \u001b[39m=\u001b[39m [pos_label]\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/ecg/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=1360'>1361</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label. It should be one of [0, 13]"
     ]
    }
   ],
   "source": [
    "Trainer(args, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation model (make submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 make dataset1 testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "해당 셀의 코드 중 dataloader 부분의 `shuffle=False` 는 수정하면 안됩니다.\n",
    "\"\"\"\n",
    "\n",
    "dataset1_dir = '/home/mgl/project_src/2022devmatch/dataset/dataset1/'\n",
    "dataset1_test_dir = dataset1_dir + 'test/'\n",
    "\n",
    "dataset1_test_X = []\n",
    "test_list = os.listdir(dataset1_test_dir)\n",
    "for f in test_list:\n",
    "  temp = pd.read_csv(dataset1_test_dir + f)\n",
    "  dataset1_test_X.append(torch.from_numpy(temp.values))\n",
    "\n",
    "dataset1_test_X = pad_sequence(dataset1_test_X, batch_first=True)\n",
    "dataset1_test_dataset = TensorDataset(torch.Tensor(dataset1_test_X))\n",
    "dataset1_test_dataloader = DataLoader(dataset1_test_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset1의 test 데이터를 사용한 일반화 성능 확인\n",
    "\n",
    "model.eval()\n",
    "predicted = []\n",
    "with torch.no_grad():\n",
    "  for idx, x in enumerate(dataset1_test_dataloader):\n",
    "    x = x.permute(0, 2, 1).contiguous().to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x.float())\n",
    "\n",
    "    _, preds = torch.max(output, 1)\n",
    "    predicted.extend(preds.cpu().numpy())\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd_preds = pd.DataFrame(predicted, columns=['predicted value'])\n",
    "pd_preds.to_csv('submission.csv')\n",
    "pd_preds.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "625ed975c5c227b3f2eae146e27cca2a4bb3771cd391bb2881da7d9db4168805"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
